{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8986047e-b3dc-4a31-a9e7-a995b82f162b",
   "metadata": {},
   "source": [
    "# CS 634 Midterm Project Apriori, FP Growth and Brute Force Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c02e39b-33d5-4722-b459-27f5eb9b419f",
   "metadata": {},
   "source": [
    "Student: Sebastian Skubisz<br> UCID: ss365<br> Instructor: Dr. Yasser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca1fafe-b754-4895-82ff-7a8f93eb8d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os, sys, csv, math, time, argparse, itertools, subprocess\n",
    "from collections import Counter\n",
    "\n",
    "# ---------- Auto-install ----------\n",
    "def ensure_package(pkg):\n",
    "    try:\n",
    "        __import__(pkg)\n",
    "    except ImportError:\n",
    "        print(f\"Installing missing package: {pkg} ...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "\n",
    "for pkg in [\"pandas\", \"tabulate\", \"mlxtend\"]:\n",
    "    ensure_package(pkg)\n",
    "\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from mlxtend.frequent_patterns import apriori as _apriori, fpgrowth as _fpgrowth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeee027-4ef9-4d10-b93d-00b893ecebe9",
   "metadata": {},
   "source": [
    "Download the required packages automatically.<br>\n",
    "If not downloaded please use the Requirements.txt is attached to zip please run by using pip install -r requirements.txt<br>\n",
    "Dataset is already included in the zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410af5d5-e3f7-4bd0-9927-ef059b6b4b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset shortcuts (key -> filename)\n",
    "DATASETS = {\n",
    "    \"amazon\": \"amazon.csv\",\n",
    "    \"microcenter\": \"microcenter.csv\",\n",
    "    \"traderjoes\": \"traderjoes.csv\",\n",
    "    \"target\": \"target.csv\",\n",
    "    \"stewleonards\": \"stewleonards.csv\"\n",
    "}\n",
    "\n",
    "# Recognized separators for single-column, tokenized baskets\n",
    "SEPS = [\",\", \";\", \"|\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc04c055-8401-4ed3-9410-fdbe5bfb8d29",
   "metadata": {},
   "source": [
    "DATASETS is a quick reference dictionary linking dataset names (keys) to their CSV filenames.\n",
    "\n",
    "SEPS lists possible separators used in CSV files to split items within a single column, allowing the script to detect and parse tokenized item lists like\"Milk, Bread, Eggs\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091f5915-27c2-4ecd-a6af-8cab8e3b3aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a tokenized item string into a list of unique, trimmed items\n",
    "def _split_items(s):\n",
    "    if not s: return []\n",
    "    s = str(s).strip()\n",
    "    for sp in SEPS:\n",
    "        if sp in s: return [t.strip() for t in s.split(sp) if t.strip()]\n",
    "    return [t.strip() for t in s.split() if t.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51dc997-6603-49b7-8df9-eb7162f3e84b",
   "metadata": {},
   "source": [
    "The _split_items() function cleans and separates items in a string.\n",
    "It removes extra spaces, detects separators like commas, semicolons, or pipes, and splits the string accordingly.\n",
    "If no separator is found, it splits by spaces.\n",
    "The result is a neat list of items, for example \"Milk, Bread, Eggs\" becomes [\"Milk\", \"Bread\", \"Eggs\"]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3749db4f-5ae8-4dbc-b9aa-553aa6e27fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heuristic to detect header rows\n",
    "def _looks_like_header(row):\n",
    "    j = \" \".join((c or \"\").lower() for c in row)\n",
    "    return any(k in j for k in [\"transaction id\",\"transaction\",\"items\",\"itemset\",\"basket\",\"products\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b06439c-78fc-4e4d-b923-ba6eb5bfa628",
   "metadata": {},
   "source": [
    "The _looks_like_header() function checks if a CSV row is likely a header.\n",
    "It combines all values in the row into one lowercase string and looks for keywords such as “transaction,” “items,” or “products.”\n",
    "If any of those words appear, it assumes the row is a header and returns True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712310db-74bf-45b1-b046-77f23cfffc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transactions from flexible CSV layouts (tokenized column, items column, or multi-column with ID)\n",
    "def load_transactions(path):\n",
    "    if not os.path.exists(path): raise FileNotFoundError(path)\n",
    "    with open(path, newline='', encoding='utf-8') as f: rows = list(csv.reader(f))\n",
    "    if not rows: return []\n",
    "    if _looks_like_header(rows[0]): rows = rows[1:]\n",
    "    rows = [[(c or \"\").strip() for c in r] for r in rows if any((c or \"\").strip() for c in r)]\n",
    "    if not rows: return []\n",
    "    ncols = max(len(r) for r in rows)\n",
    "\n",
    "    # Case 1: single column containing tokenized baskets\n",
    "    if ncols == 1: return [[*dict.fromkeys(_split_items(r[0]))] for r in rows]\n",
    "\n",
    "    # Case 2: choose the column that looks tokenized (has many separators)\n",
    "    best_c, best_hits = -1, -1\n",
    "    for c in range(ncols):\n",
    "        hits = sum(1 for r in rows if c < len(r) and any(sp in r[c] for sp in SEPS))\n",
    "        if hits > best_hits: best_c, best_hits = c, hits\n",
    "    if best_hits >= max(3, int(0.2 * len(rows))):\n",
    "        return [[*dict.fromkeys(_split_items(r[best_c] if best_c < len(r) else \"\"))] for r in rows]\n",
    "\n",
    "    # Case 3: assume first column is an ID; remaining columns are item names\n",
    "    tx = []\n",
    "    for r in rows:\n",
    "        start = 1 if (r and (r[0].replace(\"#\", \"\").isdigit() or \"id\" in r[0].lower() or \"trans\" in r[0].lower())) else 0\n",
    "        items = [c for c in r[start:] if c]\n",
    "        tx.append([*dict.fromkeys(items)])\n",
    "    return tx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01d4968-ff24-4a9c-b958-e25df93883a4",
   "metadata": {},
   "source": [
    "The load_transactions() function reads and cleans a CSV file of transactions.\n",
    "It removes headers and empty cells, then checks how the data is formatted.\n",
    "If there’s one column, it splits items in that column.\n",
    "If one column has many separators, it treats that as the item list.\n",
    "Otherwise, it assumes the first column is an ID and the rest are items, returning a clean list of transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9618efd-8d06-4247-9200-5761792227f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list-of-lists transactions into a one-hot encoded DataFrame (bool dtype)\n",
    "def to_onehot(tx):\n",
    "    items = sorted({i for t in tx for i in t})\n",
    "    return pd.DataFrame([{i: (i in set(t)) for i in items} for t in tx], dtype=bool)\n",
    "\n",
    "# Count transactions containing a given itemset (helper for brute-force)\n",
    "def _support_count(items, tx):\n",
    "    st = set(items)\n",
    "    return sum(st.issubset(t) for t in tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7615a73-5080-426f-b648-47f623d85c1d",
   "metadata": {},
   "source": [
    "The to_onehot() function converts transactions into a one-hot encoded table where each item becomes a column marked True or False.\n",
    "The _support_count() function counts how many transactions contain all items from a given itemset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e98e13-1696-42c7-8cb7-99497d4369ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate association rules from frequent itemsets with unified semantics across algorithms\n",
    "def _rules_from_fi(fi_df, conf_pct):\n",
    "    # fi_df must have columns: 'itemset' (iterable/frozenset) and 'support' (fraction 0..1)\n",
    "    if fi_df.empty:\n",
    "        return pd.DataFrame(columns=[\"antecedents\",\"consequents\",\"support\",\"confidence\"])\n",
    "    min_conf = conf_pct / 100.0\n",
    "    sup_map = {}\n",
    "    for fs, s in zip(fi_df[\"itemset\"], fi_df[\"support\"]):\n",
    "        k = fs if isinstance(fs, frozenset) else frozenset(fs)\n",
    "        sup_map[k] = float(s)\n",
    "\n",
    "    rows = []\n",
    "    for L, supL in sup_map.items():\n",
    "        if len(L) < 2:  # need at least 2 items to form A|B\n",
    "            continue\n",
    "        for r in range(1, len(L)):\n",
    "            for A in itertools.combinations(L, r):\n",
    "                A = frozenset(A)\n",
    "                B = L - A\n",
    "                supA = sup_map.get(A)\n",
    "                if not supA or supA <= 0:\n",
    "                    continue\n",
    "                conf = supL / supA\n",
    "                if conf >= min_conf:\n",
    "                    rows.append({\n",
    "                        \"antecedents\": A,\n",
    "                        \"consequents\": B,\n",
    "                        \"support\": supL,\n",
    "                        \"confidence\": conf\n",
    "                    })\n",
    "    return pd.DataFrame(rows, columns=[\"antecedents\",\"consequents\",\"support\",\"confidence\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eb4ad5-ec53-490e-9a27-ffcffc3d569b",
   "metadata": {},
   "source": [
    "The _rules_from_fi() function creates association rules from frequent itemsets.\n",
    "It checks each itemset that meets the minimum confidence level and splits it into possible rule pairs (A → B).\n",
    "For each pair, it calculates support and confidence, keeping only those that meet the confidence threshold.\n",
    "The result is a table of rules showing which items are likely to be bought together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f1d26b-e1f2-4e3c-9500-a5cdd2215fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brute-force miner (custom): enumerate all item combinations meeting minsup; then build rules\n",
    "def brute_force(tx, sup_pct, conf_pct):\n",
    "    t0 = time.perf_counter()\n",
    "    n = len(tx)\n",
    "    if n == 0:\n",
    "        return (pd.DataFrame(columns=[\"itemset\",\"support\"]),\n",
    "                pd.DataFrame(columns=[\"antecedents\",\"consequents\",\"support\",\"confidence\"]),\n",
    "                0.0)\n",
    "\n",
    "    min_sup = max(1, math.ceil(sup_pct / 100 * n))\n",
    "    min_conf = conf_pct / 100\n",
    "    all_items = sorted({i for t in tx for i in t})\n",
    "\n",
    "    freq = {}  # map frozenset -> support count\n",
    "    k = 1\n",
    "    while True:\n",
    "        found = False\n",
    "        for comb in itertools.combinations(all_items, k):\n",
    "            cnt = _support_count(comb, tx)\n",
    "            if cnt >= min_sup:\n",
    "                freq[frozenset(comb)] = cnt\n",
    "                found = True\n",
    "        if not found:\n",
    "            break\n",
    "        k += 1\n",
    "\n",
    "    fi_rows = [{\"itemset\": fs, \"support\": c/n} for fs, c in freq.items()]\n",
    "    fi_df = pd.DataFrame(fi_rows) if fi_rows else pd.DataFrame(columns=[\"itemset\",\"support\"])\n",
    "\n",
    "    # Rules with same logic as _rules_from_fi to ensure parity\n",
    "    rules = []\n",
    "    sup_map = {fs: c/n for fs, c in freq.items()}\n",
    "    for L, supL in sup_map.items():\n",
    "        if len(L) < 2:\n",
    "            continue\n",
    "        for r in range(1, len(L)):\n",
    "            for A in itertools.combinations(L, r):\n",
    "                A = frozenset(A)\n",
    "                B = L - A\n",
    "                supA = sup_map.get(A, 0.0)\n",
    "                if supA <= 0:\n",
    "                    continue\n",
    "                conf = supL / supA\n",
    "                if conf >= min_conf:\n",
    "                    rules.append({\n",
    "                        \"antecedents\": A,\n",
    "                        \"consequents\": B,\n",
    "                        \"support\": supL,\n",
    "                        \"confidence\": conf\n",
    "                    })\n",
    "    rules_df = pd.DataFrame(rules, columns=[\"antecedents\",\"consequents\",\"support\",\"confidence\"])\n",
    "    dt = time.perf_counter() - t0\n",
    "    return fi_df, rules_df, dt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e247b98f-320c-43d0-b4cc-139e5ee37c72",
   "metadata": {},
   "source": [
    "The brute_force() function finds frequent itemsets and rules by checking every possible item combination.\n",
    "It counts how often each combination appears and keeps those that meet the minimum support.\n",
    "Then, it creates rules from these itemsets and keeps only the ones meeting the confidence level.\n",
    "Finally, it returns the frequent itemsets, rules, and the time it took to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebf83ee-77c8-4694-a623-fcfee899b988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apriori miner via mlxtend (find frequent itemsets; rules created by shared rule generator)\n",
    "def apriori(df_onehot, sup_pct, conf_pct):\n",
    "    t0 = time.perf_counter()\n",
    "    sup = sup_pct / 100.0\n",
    "    fi = _apriori(df_onehot, min_support=sup, use_colnames=True)\n",
    "    if fi.empty:\n",
    "        return (pd.DataFrame(columns=[\"itemset\",\"support\"]),\n",
    "                pd.DataFrame(columns=[\"antecedents\",\"consequents\",\"support\",\"confidence\"]),\n",
    "                time.perf_counter() - t0)\n",
    "    fi = fi.rename(columns={\"itemsets\":\"itemset\"})[[\"itemset\",\"support\"]]\n",
    "    rules = _rules_from_fi(fi, conf_pct)\n",
    "    dt = time.perf_counter() - t0\n",
    "    return fi, rules, dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eef53e-959a-4b51-84b1-a2672d92cb9b",
   "metadata": {},
   "source": [
    "The apriori() function uses the Apriori algorithm to find frequent itemsets from the one-hot encoded data.\n",
    "It filters itemsets based on the minimum support and then generates rules using the shared rule function.\n",
    "Finally, it returns the frequent itemsets, the generated rules, and the total execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9968d9b-afde-4e3f-b19b-602f6e1b6c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP-Growth miner via mlxtend (find frequent itemsets; rules created by shared rule generator)\n",
    "def fpgrowth(df_onehot, sup_pct, conf_pct):\n",
    "    t0 = time.perf_counter()\n",
    "    sup = sup_pct / 100.0\n",
    "    fi = _fpgrowth(df_onehot, min_support=sup, use_colnames=True)\n",
    "    if fi.empty:\n",
    "        return (pd.DataFrame(columns=[\"itemset\",\"support\"]),\n",
    "                pd.DataFrame(columns=[\"antecedents\",\"consequents\",\"support\",\"confidence\"]),\n",
    "                time.perf_counter() - t0)\n",
    "    fi = fi.rename(columns={\"itemsets\":\"itemset\"})[[\"itemset\",\"support\"]]\n",
    "    rules = _rules_from_fi(fi, conf_pct)\n",
    "    dt = time.perf_counter() - t0\n",
    "    return fi, rules, dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3732932-3961-44e6-b4a5-af6c4b5e8433",
   "metadata": {},
   "source": [
    "The fpgrowth() function uses the FP-Growth algorithm to quickly find frequent itemsets from the one-hot encoded data.\n",
    "It keeps only itemsets that meet the minimum support and then generates rules using the shared rule function.\n",
    "Finally, it returns the frequent itemsets, generated rules, and the time taken to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b869b44-eee9-40f1-a151-88e18ca4ea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert itemsets to readable strings for printing/saving\n",
    "def _format_itemset(x):\n",
    "    if isinstance(x, (set, frozenset, list, tuple)):\n",
    "        return \", \".join(sorted(map(str, x)))\n",
    "    try:\n",
    "        return \", \".join(sorted(map(str, list(x))))\n",
    "    except Exception:\n",
    "        return str(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09da54f0-186a-46e3-995b-d5a87901337e",
   "metadata": {},
   "source": [
    "The _format_itemset() function converts a collection of items (like a set or list) into a readable string.\n",
    "It sorts and joins all items with commas, such as turning {\"Milk\", \"Bread\"} into \"Bread, Milk\".\n",
    "If formatting fails, it simply returns the item as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a82f44-16dc-44eb-bd8e-6f67ae8bd6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty-print association rules sorted by confidence then support\n",
    "def print_rules_table(df, title):\n",
    "    print(f\"\\n{title}\")\n",
    "    if df.empty:\n",
    "        print(\"(no rules)\")\n",
    "        return\n",
    "    d = df.sort_values([\"confidence\",\"support\"], ascending=False).copy()\n",
    "    d[\"antecedents\"] = d[\"antecedents\"].apply(_format_itemset)\n",
    "    d[\"consequents\"] = d[\"consequents\"].apply(_format_itemset)\n",
    "    d[\"support\"] = d[\"support\"].map(lambda v: f\"{v:.2f}\")\n",
    "    d[\"confidence\"] = d[\"confidence\"].map(lambda v: f\"{v:.2f}\")\n",
    "    print(tabulate(d[[\"antecedents\",\"consequents\",\"support\",\"confidence\"]],\n",
    "                   headers=[\"Antecedent(s)\",\"Consequent(s)\",\"Support\",\"Confidence\"],\n",
    "                   tablefmt=\"github\", showindex=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40392823-bdd2-40dd-a1e2-c07823d43071",
   "metadata": {},
   "source": [
    "The print_rules_table() function neatly displays all association rules in a table format.\n",
    "It sorts the rules by confidence and support, formats itemsets for readability, and rounds the values.\n",
    "If no rules are found, it simply prints “(no rules)”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5776adab-0c5a-4b65-bd06-088d53a48187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty-print frequent itemsets sorted by support\n",
    "def print_itemsets_table(df, title):\n",
    "    print(f\"\\n{title}\")\n",
    "    if df.empty:\n",
    "        print(\"(no frequent itemsets)\")\n",
    "        return\n",
    "    d = df.copy().sort_values([\"support\"], ascending=[False])\n",
    "    d[\"itemset\"] = d[\"itemset\"].apply(_format_itemset)\n",
    "    d[\"support\"] = d[\"support\"].map(lambda v: f\"{v:.2f}\")\n",
    "    print(tabulate(d[[\"itemset\",\"support\"]], headers=[\"Itemset\",\"Support\"],\n",
    "                   tablefmt=\"github\", showindex=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0d3846-31ef-41b3-91cd-cce10797708f",
   "metadata": {},
   "source": [
    "The print_itemsets_table() function displays all frequent itemsets in a clear table.\n",
    "It sorts them by support in descending order, formats the item names for readability, and rounds the support values.\n",
    "If no itemsets are found, it prints “(no frequent itemsets)”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68082254-666a-4274-9d48-751f6717a6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save frequent itemsets to CSV (formats itemset lists into strings)\n",
    "def save_csv_itemsets(df, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    out = df.copy()\n",
    "    out[\"itemset\"] = out[\"itemset\"].apply(_format_itemset)\n",
    "    out = out[[\"itemset\",\"support\"]] if \"support\" in out.columns else out[[\"itemset\"]]\n",
    "    out.to_csv(path, index=False)\n",
    "\n",
    "# Save association rules to CSV (antecedents/consequents as strings)\n",
    "def save_csv_rules(df, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    out = df.copy()\n",
    "    out[\"antecedents\"] = out[\"antecedents\"].apply(_format_itemset)\n",
    "    out[\"consequents\"] = out[\"consequents\"].apply(_format_itemset)\n",
    "    out = out[[\"antecedents\",\"consequents\",\"support\",\"confidence\"]]\n",
    "    out.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8608c2e-8e93-4965-9f40-b00f26300159",
   "metadata": {},
   "source": [
    "The save_csv_itemsets() function saves frequent itemsets to a CSV file, converting each itemset into a readable string before writing.\n",
    "The save_csv_rules() function does the same for association rules, formatting the antecedents and consequents, then saving them along with their support and confidence values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29313e43-342b-416c-9c42-d794fa9cff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize user-provided percentage/fraction to percentage\n",
    "def normalize_pct(value, default_if_none):\n",
    "    if value is None:\n",
    "        return float(default_if_none)\n",
    "    v = float(value)\n",
    "    if 0.0 <= v <= 1.0:\n",
    "        return v * 100.0\n",
    "    return v\n",
    "    \n",
    "def _pct_arg(s: str) -> float:\n",
    "    \"\"\"Accept 0..1 (fraction) or 1..100 (percent), but disallow 0.\"\"\"\n",
    "    try:\n",
    "        v = float(s)\n",
    "    except ValueError:\n",
    "        raise argparse.ArgumentTypeError(\"Must be a number.\")\n",
    "    if (0 < v <= 1) or (1 < v <= 100) or v == 1.0:\n",
    "        return v\n",
    "    raise argparse.ArgumentTypeError(\"Use (0,1] for fraction or (0,100] for percent; 0 is not allowed.\")\n",
    "\n",
    "# Prompt for minsup/minconf if omitted\n",
    "def prompt_pct(label: str, default: float) -> float:\n",
    "    while True:\n",
    "        s = input(f\"{label} (1-100 or 0..1) [{default}]: \").strip()\n",
    "        if s == \"\":\n",
    "            return float(default)\n",
    "        try:\n",
    "            v = float(s)\n",
    "            # disallow 0; allow (0,1] or (0,100]\n",
    "            if (0 < v <= 100) or (0 < v <= 1):\n",
    "                return v\n",
    "        except Exception:\n",
    "            pass\n",
    "        print(\"Please enter a number in (0,100] or (0,1] (e.g., 0.4 or 40).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e382f0b-ee21-4ade-914c-59293db14b57",
   "metadata": {},
   "source": [
    "normalize_pct() converts user input into a percentage. If the value is between 0 and 1, it multiplies by 100; otherwise, it returns the number as-is.\n",
    "\n",
    "_pct_arg() checks that the input is a valid number greater than 0 and within 1–100 (percent) or 0–1 (fraction). It prevents users from entering 0 or invalid values.\n",
    "\n",
    "prompt_pct() asks the user to enter a support or confidence value. It repeats the prompt until the user gives a valid number greater than 0 within the allowed range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3165b6db-aa67-4d1d-a65c-536ea7b5990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive dataset chooser for when --dataset is omitted\n",
    "def choose_dataset():\n",
    "    keys = list(DATASETS.keys())\n",
    "    while True:\n",
    "        print(\"Choose a dataset:\")\n",
    "        for i,k in enumerate(keys,1):\n",
    "            print(f\" {i}. {k.title()} ({DATASETS[k]})\")\n",
    "        sel = input(f\"Enter number (1-{len(keys)}): \").strip()\n",
    "        if sel.isdigit() and 1 <= int(sel) <= len(keys):\n",
    "            key = keys[int(sel)-1]; path = DATASETS[key]\n",
    "            if os.path.exists(path): return key, path\n",
    "            print(f\"File not found: {path}\")\n",
    "        else:\n",
    "            print(f\"Invalid choice. Enter 1–{len(keys)}.\")\n",
    "\n",
    "# Merge frequent itemsets from multiple methods, keeping max support per unique itemset\n",
    "def consolidate_itemsets(*dfs):\n",
    "    parts = [df[[\"itemset\",\"support\"]].copy() for df in dfs if df is not None and not df.empty]\n",
    "    if not parts:\n",
    "        return pd.DataFrame(columns=[\"itemset\",\"support\",\"len\"])\n",
    "    cat = pd.concat(parts, ignore_index=True)\n",
    "    cat[\"key\"] = cat[\"itemset\"].apply(lambda x: frozenset(x) if not isinstance(x, frozenset) else x)\n",
    "    agg = (cat.groupby(\"key\", as_index=False).agg({\"support\":\"max\"}))\n",
    "    agg[\"itemset\"] = agg[\"key\"].apply(lambda k: k)\n",
    "    agg[\"len\"] = agg[\"itemset\"].apply(len)\n",
    "    return agg[[\"itemset\",\"support\",\"len\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a16af73-272f-4f37-9f2e-bdd40f347645",
   "metadata": {},
   "source": [
    "The choose_dataset() function asks the user to pick which dataset to use when none is given through the command line.\n",
    "It displays all available dataset options, waits for the user to enter a number, and returns the selected dataset’s name and file path once confirmed.\n",
    "\n",
    "The consolidate_itemsets() function combines frequent itemsets from all algorithms.\n",
    "It merges duplicates, keeps the highest support value for each unique itemset, and adds the itemset length for easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2cd522-4dbb-4d13-b81e-9330bd1bce41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orchestrate the full pipeline: load, mine (3 methods), print, and save outputs\n",
    "def run_all(ds_key, path, sup_in, conf_in):\n",
    "    sup_pct  = normalize_pct(sup_in, 20)\n",
    "    conf_pct = normalize_pct(conf_in, 50)\n",
    "\n",
    "    tx = load_transactions(path)\n",
    "    n, uniq = len(tx), Counter(i for t in tx for i in t)\n",
    "    need = math.ceil(sup_pct/100 * n) if n else 0\n",
    "\n",
    "    print(f\"\\n===== {ds_key.title()} =====\")\n",
    "    print(f\"Loaded {n} transactions, {len(uniq)} unique items. \"\n",
    "          f\"Min Support {sup_pct:.0f}% (>= {need}/{n}), Min Confidence {conf_pct:.0f}%\")\n",
    "    if uniq:\n",
    "        freqs = pd.DataFrame(sorted(uniq.items(), key=lambda x: (-x[1], x[0])), columns=[\"Item\",\"Count\"])\n",
    "        print(tabulate(freqs, headers=\"keys\", tablefmt=\"github\", showindex=False))\n",
    "\n",
    "    df_onehot = to_onehot(tx)\n",
    "\n",
    "    # Prepare output directories per algorithm\n",
    "    root_out = os.path.join(\"outputs\", ds_key)\n",
    "    out_bf = os.path.join(root_out, \"Brute-Force\")\n",
    "    out_ap = os.path.join(root_out, \"Apriori\")\n",
    "    out_fp = os.path.join(root_out, \"FP-Growth\")\n",
    "    os.makedirs(root_out, exist_ok=True)\n",
    "\n",
    "    timings = []\n",
    "\n",
    "    # Brute-Force mining + CSV export\n",
    "    bf_itemsets, bf_rules, t = brute_force(tx, sup_pct, conf_pct)\n",
    "    timings.append((\"Brute-Force\", int(len(bf_itemsets)), int(len(bf_rules)), round(t,4)))\n",
    "    save_csv_itemsets(bf_itemsets, os.path.join(out_bf, \"frequent_itemsets.csv\"))\n",
    "    save_csv_rules(bf_rules,    os.path.join(out_bf, \"association_rules.csv\"))\n",
    "\n",
    "    # Apriori mining + CSV export\n",
    "    ap_itemsets, ap_rules, t = apriori(df_onehot, sup_pct, conf_pct)\n",
    "    timings.append((\"Apriori\", int(len(ap_itemsets)), int(len(ap_rules)), round(t,4)))\n",
    "    save_csv_itemsets(ap_itemsets, os.path.join(out_ap, \"frequent_itemsets.csv\"))\n",
    "    save_csv_rules(ap_rules,    os.path.join(out_ap, \"association_rules.csv\"))\n",
    "\n",
    "    # FP-Growth mining + CSV export\n",
    "    fp_itemsets, fp_rules, t = fpgrowth(df_onehot, sup_pct, conf_pct)\n",
    "    timings.append((\"FP-Growth\", int(len(fp_itemsets)), int(len(fp_rules)), round(t,4)))\n",
    "    save_csv_itemsets(fp_itemsets, os.path.join(out_fp, \"frequent_itemsets.csv\"))\n",
    "    save_csv_rules(fp_rules,    os.path.join(out_fp, \"association_rules.csv\"))\n",
    "\n",
    "    # Print consolidated itemsets and per-method rule tables\n",
    "    consolidated = consolidate_itemsets(bf_itemsets, ap_itemsets, fp_itemsets)\n",
    "    print_itemsets_table(consolidated, \"Frequent Itemsets (Consolidated - all)\")\n",
    "    print_rules_table(bf_rules, \"Brute-Force: Association Rules (all)\")\n",
    "    print_rules_table(ap_rules, \"Apriori: Association Rules (all)\")\n",
    "    print_rules_table(fp_rules, \"FP-Growth: Association Rules (all)\")\n",
    "\n",
    "    # Print and save timing summary CSV\n",
    "    tdf = pd.DataFrame(timings, columns=[\"Algorithm\",\"Frequent Itemsets\",\"Rules\",\"Time (s)\"])\n",
    "    print(\"\\nTiming Summary:\")\n",
    "    print(tabulate(tdf, headers=\"keys\", tablefmt=\"github\", showindex=False))\n",
    "    tdf.to_csv(os.path.join(root_out, \"timings.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20320a01-6788-4fff-be8f-1d5734b45ee9",
   "metadata": {},
   "source": [
    "The run_all() function executes the whole workflow for one dataset.\n",
    "It normalizes thresholds, loads and summarizes the data, and builds a one-hot table.\n",
    "Then it runs Brute-Force, Apriori, and FP-Growth, saving itemsets and rules for each.\n",
    "It prints consolidated itemsets and per-method rule tables.\n",
    "Finally, it outputs a timing summary and saves it to ./outputs/<dataset>/timings.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cf2dcb-f34d-427a-8dcb-9e1516ff98a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main(argv=None):\n",
    "    p = argparse.ArgumentParser(\n",
    "        description=\"Run Brute-Force, Apriori, FP-Growth on one dataset; prints consolidated itemsets and per-alg rules.\"\n",
    "    )\n",
    "    p.add_argument(\"--dataset\", choices=list(DATASETS.keys()))\n",
    "    p.add_argument(\"--minsup\", type=float)\n",
    "    p.add_argument(\"--minconf\", type=float)\n",
    "\n",
    "    if argv is None:\n",
    "        argv = [] if 'ipykernel' in sys.modules else sys.argv[1:]\n",
    "\n",
    "    a, _ = p.parse_known_args(argv)  # <-- ignore Jupyter's extra args\n",
    "\n",
    "    if a.dataset:\n",
    "        key, path = a.dataset, DATASETS[a.dataset]\n",
    "    else:\n",
    "        key, path = choose_dataset()\n",
    "\n",
    "    sup_in  = a.minsup  if a.minsup  is not None else prompt_pct(\"Minimum support\", 20)\n",
    "    conf_in = a.minconf if a.minconf is not None else prompt_pct(\"Minimum confidence\", 50)\n",
    "    run_all(key, path, sup_in, conf_in)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3202df53-afa5-4f59-97c9-b4be507cd2da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40eb01a6-725a-4814-84f2-3388ae2251ac",
   "metadata": {},
   "source": [
    "It lets the program accept command-line inputs for dataset, minimum support, and confidence.\n",
    "If no arguments are given, it asks the user interactively.\n",
    "It chooses the dataset, gets support and confidence values, and then runs all algorithms.\n",
    "The if __name__ == \"__main__\": main() line runs the function when the script is executed directly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
